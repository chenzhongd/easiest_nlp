{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba85684",
   "metadata": {},
   "source": [
    "### NLP建模，可用于科研实验、企业建模\n",
    "#### 功能包括：\n",
    "- 1.数据预处理\n",
    "- 2.数据集构建\n",
    "- 3.预训练词向量应用\n",
    "- 4.模型训练、评估、预测\n",
    "- 5.模型线上化，保存java可调用模型文件\n",
    "- 6.精细化模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "808504cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenzhong72/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# coding: UTF-8\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3a65f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"dataset/train.txt\",sep = \"\\t\")\n",
    "test_df = pd.read_csv(\"dataset/test.txt\",sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e41285bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8564\n",
       "1    4236\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3e38ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2115\n",
       "1    1085\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51830962",
   "metadata": {},
   "source": [
    "## 数据集切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6e18f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train,valdate  = train_test_split(train_df, test_size = 0.2,random_state = 110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72a5ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"data_category\"]  = \"train\"\n",
    "valdate[\"data_category\"]  = \"valdate\"\n",
    "test_df[\"data_category\"]  = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3befd417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zn/52tvsv3d51zc1jvgg_8vq2bw0000gp/T/ipykernel_32077/1770443426.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = train.append(valdate).append(test_df)\n",
      "/var/folders/zn/52tvsv3d51zc1jvgg_8vq2bw0000gp/T/ipykernel_32077/1770443426.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = train.append(valdate).append(test_df)\n"
     ]
    }
   ],
   "source": [
    "data = train.append(valdate).append(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f480c",
   "metadata": {},
   "source": [
    "# 1、数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43451b2e",
   "metadata": {},
   "source": [
    "## 1.1 Unicode Nomalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9db3710b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'GeeksForGeeks ????'\n"
     ]
    }
   ],
   "source": [
    "# 暂无需使用\n",
    "\n",
    "text = \"GeeksForGeeks ????\"\n",
    "print(text.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f512cd3c",
   "metadata": {},
   "source": [
    "## 1.2 剔除字符空格数字等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57615001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def data_re(sentence):\n",
    "    # 过滤不了\\\\ \\ 中文（）还有————\n",
    "    r1 = u'[a-zA-Z0-9’!\"#$%&\\'()*+,-./:;<=>?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~]+'#用户也可以在此进行自定义过滤字符 \n",
    "    # 者中规则也过滤不完全\n",
    "    r2 = \"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\"\n",
    "    # \\\\\\可以过滤掉反向单杠和双杠，/可以过滤掉正向单杠和双杠，第一个中括号里放的是英文符号，第二个中括号里放的是中文符号，第二个中括号前不能少|，否则过滤不完全\n",
    "    r3 =  \"[.!//_,$&%^*()<>+\\\"'?@#-|:~{}]+|[——！\\\\\\\\，。=？、：“”‘’《》【】￥……（）]+\" \n",
    "    # 去掉括号和括号内的所有内容\n",
    "    r4 =  \"\\\\【.*?】+|\\\\《.*?》+|\\\\#.*?#+|[.!/_,$&%^*<>+\"\"'?@|:~{}#]+|[——！\\\\\\，。=？、：“”‘’￥……（）《》【】]\"\n",
    "\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    sentence = re.sub(cleanr, ' ', sentence)        #去除html标签\n",
    "\n",
    "    \n",
    "    sentence = re.sub(r1,'',sentence)\n",
    "    sentence = re.sub(r2,'',sentence)\n",
    "    sentence = re.sub(r4,'',sentence)\n",
    "    \n",
    "    \n",
    "#     sentence = re.sub(r\"\\d+\", '',sentence)  # 仅去除数字 d即为数字\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24ab9026",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] = data['text'].map(lambda x :x if pd.isnull(x) else data_re(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62022c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1745     味道一如既往的好吃\n",
       "7529      东西一如既往的好\n",
       "11487     生日快到的前几天\n",
       "4410       鹅肝寿司是最爱\n",
       "10422     面包烤的恰到好处\n",
       "Name: text_clean, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text_clean'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7817a",
   "metadata": {},
   "source": [
    "## 1.3 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6a9bacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def cut_word(x):\n",
    "    if pd.isnull(x):\n",
    "        return x\n",
    "    else:\n",
    "        cw = jieba.cut(x)\n",
    "        return list(cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5c1351f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/zn/52tvsv3d51zc1jvgg_8vq2bw0000gp/T/jieba.cache\n",
      "Loading model cost 0.653 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "data['clean_cut_word'] = data['text_clean'].apply(cut_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fce4e8",
   "metadata": {},
   "source": [
    "## 1.4 剔除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3c52098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "stop_words = stopwords.words('chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecc23e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_restopwords'] = data['clean_cut_word'].map(lambda x :[item for item in x \n",
    "                                                                            if item  not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55a31c7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>data_category</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>clean_cut_word</th>\n",
       "      <th>text_restopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>0</td>\n",
       "      <td>味道一如既往的好吃</td>\n",
       "      <td>train</td>\n",
       "      <td>味道一如既往的好吃</td>\n",
       "      <td>[味道, 一如既往, 的, 好吃]</td>\n",
       "      <td>[味道, 一如既往, 好吃]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7529</th>\n",
       "      <td>0</td>\n",
       "      <td>东西一如既往的好</td>\n",
       "      <td>train</td>\n",
       "      <td>东西一如既往的好</td>\n",
       "      <td>[东西, 一如既往, 的, 好]</td>\n",
       "      <td>[东西, 一如既往, 好]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11487</th>\n",
       "      <td>0</td>\n",
       "      <td>生日快到的前几天</td>\n",
       "      <td>train</td>\n",
       "      <td>生日快到的前几天</td>\n",
       "      <td>[生日, 快到, 的, 前, 几天]</td>\n",
       "      <td>[生日, 快到, 前, 几天]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4410</th>\n",
       "      <td>1</td>\n",
       "      <td>鹅肝寿司是最爱</td>\n",
       "      <td>train</td>\n",
       "      <td>鹅肝寿司是最爱</td>\n",
       "      <td>[鹅, 肝, 寿司, 是, 最, 爱]</td>\n",
       "      <td>[鹅, 肝, 寿司, 最, 爱]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10422</th>\n",
       "      <td>1</td>\n",
       "      <td>面包烤的恰到好处</td>\n",
       "      <td>train</td>\n",
       "      <td>面包烤的恰到好处</td>\n",
       "      <td>[面包, 烤, 的, 恰到好处]</td>\n",
       "      <td>[面包, 烤, 恰到好处]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label       text data_category text_clean       clean_cut_word  \\\n",
       "1745       0  味道一如既往的好吃         train  味道一如既往的好吃    [味道, 一如既往, 的, 好吃]   \n",
       "7529       0   东西一如既往的好         train   东西一如既往的好     [东西, 一如既往, 的, 好]   \n",
       "11487      0   生日快到的前几天         train   生日快到的前几天   [生日, 快到, 的, 前, 几天]   \n",
       "4410       1    鹅肝寿司是最爱         train    鹅肝寿司是最爱  [鹅, 肝, 寿司, 是, 最, 爱]   \n",
       "10422      1   面包烤的恰到好处         train   面包烤的恰到好处     [面包, 烤, 的, 恰到好处]   \n",
       "\n",
       "       text_restopwords  \n",
       "1745     [味道, 一如既往, 好吃]  \n",
       "7529      [东西, 一如既往, 好]  \n",
       "11487   [生日, 快到, 前, 几天]  \n",
       "4410   [鹅, 肝, 寿司, 最, 爱]  \n",
       "10422     [面包, 烤, 恰到好处]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7271f604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train      10240\n",
       "test        3200\n",
       "valdate     2560\n",
       "Name: data_category, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data_category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becef612",
   "metadata": {},
   "source": [
    "#### 当前建模文本采用【剔除字符空格数字】处理的文本，即text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "459bc288",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_features = [\"text_clean\",\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23b43c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in set(data.data_category):\n",
    "    data[data[\"data_category\"]==i][save_features].to_csv(\"dataset/after_process_\"+i+\".csv\",\n",
    "                                                         index = False, sep = \"\\t\",header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a443d",
   "metadata": {},
   "source": [
    "# 2、构建预训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec0e655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"dataset/after_process_train.csv\"\n",
    "val_path = \"dataset/after_process_valdate.csv\"\n",
    "test_path = \"dataset/after_process_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7012bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(file_path, tokenizer, max_size, min_freq):\n",
    "    vocab_dic = {}\n",
    "    with open(file_path, 'r', encoding='UTF-8') as f:\n",
    "        for line in tqdm(f):\n",
    "            lin = line.strip()\n",
    "            if not lin:\n",
    "                continue\n",
    "            content = lin.split('\\t')[0]\n",
    "            for word in tokenizer(content):\n",
    "                vocab_dic[word] = vocab_dic.get(word, 0) + 1\n",
    "\n",
    "        vocab_list = sorted([_ for _ in vocab_dic.items() if _[1] >= min_freq], key=lambda x: x[1], reverse=True)[:max_size]\n",
    "        vocab_dic = {word_count[0]: idx for idx, word_count in enumerate(vocab_list)}\n",
    "        vocab_dic.update({UNK: len(vocab_dic), PAD: len(vocab_dic) + 1})\n",
    "    return vocab_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64970e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 10000  # 词表长度限制\n",
    "embedding_size = 300\n",
    "UNK, PAD = '<UNK>', '<PAD>'  # 未知字，padding符号\n",
    "sequence_length = 128\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "# 下面的目录、文件名按需更改。\n",
    "train_dir = \"dataset/after_process_train.csv\"\n",
    "vocab_dir = \"Word2vec/Word2vec.pkl\"\n",
    "pretrain_dir = \"Word2vec/sgns.wiki.bigram-char\"\n",
    "filename_trimmed_dir = \"Word2vec/Word2vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65cf386f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****start*****\n",
      "不存在vocab_dir，将创建词表\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10240it [00:00, 307121.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****done!*****\n"
     ]
    }
   ],
   "source": [
    "print(\"*****start*****\")\n",
    "if os.path.exists(vocab_dir):\n",
    "    print(\"存在vocab_dir，将直接加载词表\")\n",
    "    word_to_id = pkl.load(open(vocab_dir, 'rb'))\n",
    "else:\n",
    "    print(\"不存在vocab_dir，将创建词表\")\n",
    "    # tokenizer = lambda x: x.split(' ')  # 以词为单位构建词表(数据集中词之间以空格隔开)\n",
    "    tokenizer = lambda x: [y for y in x]  # 以字为单位构建词表\n",
    "    word_to_id = build_vocab(train_dir, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=1)\n",
    "    pkl.dump(word_to_id, open(vocab_dir, 'wb'))\n",
    "\n",
    "embeddings = np.random.rand(len(word_to_id), embedding_size)\n",
    "f = open(pretrain_dir, \"r\", encoding='UTF-8')\n",
    "for i, line in enumerate(f.readlines()):\n",
    "    if i == 0:  # 若第一行是标题，则跳过\n",
    "        continue\n",
    "    lin = line.strip().split(\" \")\n",
    "    if lin[0] in word_to_id:\n",
    "        idx = word_to_id[lin[0]]\n",
    "        emb = [float(x) for x in lin[1:301]]\n",
    "        embeddings[idx] = np.asarray(emb, dtype='float32')\n",
    "f.close()\n",
    "np.savez_compressed(filename_trimmed_dir, embeddings=embeddings)\n",
    "print(\"*****done!*****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f055ace2",
   "metadata": {},
   "source": [
    "# 4、训练数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d367f5",
   "metadata": {},
   "source": [
    "## 4.1 dataset构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "846e648a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_path为 Word2vec/Word2vec.pkl\n",
      "Vocab size: 2270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10240it [00:00, 41254.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_path为 Word2vec/Word2vec.pkl\n",
      "Vocab size: 2270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2560it [00:00, 39577.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_path为 Word2vec/Word2vec.pkl\n",
      "Vocab size: 2270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3200it [00:00, 41832.32it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_file = filename_trimmed_dir + \".npz\"\n",
    "\n",
    "embedding_pretrained = torch.tensor(np.load(embeddings_file)[\"embeddings\"].astype('float32'))\n",
    "embedding = nn.Embedding.from_pretrained(embedding_pretrained, freeze=False)\n",
    "\n",
    "word_to_id = pkl.load(open(vocab_dir, 'rb'))\n",
    "\n",
    "MAX_VOCAB_SIZE = 10000  # 词表长度限制\n",
    "UNK, PAD = '<UNK>', '<PAD>'  # 未知字，padding符号\n",
    "\n",
    "def build_dataset(train_path, vocab_path,ues_word,pad_size): \n",
    "    print(\"vocab_path为\",vocab_path)\n",
    "    if ues_word:\n",
    "        tokenizer = lambda x: x.split(' ')  # 以空格隔开，word-level\n",
    "    else:\n",
    "        tokenizer = lambda x: [y for y in x]  # char-level\n",
    "    if os.path.exists(vocab_path):\n",
    "        vocab = pkl.load(open(vocab_path, 'rb'))\n",
    "    else:\n",
    "        vocab = build_vocab(train_path, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=1)\n",
    "        pkl.dump(vocab, open(vocab_path, 'wb'))\n",
    "    print(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "    def load_dataset(path, pad_size=32):\n",
    "        contents = []\n",
    "        with open(path, 'r', encoding='UTF-8') as f:\n",
    "            for line in tqdm(f):\n",
    "                lin = line.strip()\n",
    "                if (not lin) or (len(lin.split('\\t'))==1):\n",
    "                    continue\n",
    "                content, label = lin.split('\\t')\n",
    "                words_line = []\n",
    "                token = tokenizer(content)\n",
    "                seq_len = len(token)\n",
    "                if pad_size:\n",
    "                    if len(token) < pad_size:\n",
    "                        token.extend([PAD] * (pad_size - len(token)))\n",
    "                    else:\n",
    "                        token = token[:pad_size]\n",
    "                        seq_len = pad_size\n",
    "                # word to id\n",
    "                for word in token:\n",
    "                    words_line.append(vocab.get(word, vocab.get(UNK)))\n",
    "                contents.append((words_line, int(label), seq_len))\n",
    "        return contents  # [([...], 0), ([...], 1), ...]\n",
    "    train = load_dataset(train_path, pad_size)\n",
    "    return vocab, train\n",
    "\n",
    "\n",
    "train_vocab, train_data = build_dataset(train_path,vocab_path = vocab_dir,ues_word = False,pad_size = sequence_length)\n",
    "val_vocab, val_data = build_dataset(val_path,vocab_path = vocab_dir,ues_word = False,pad_size = sequence_length)\n",
    "test_vocab, test_data = build_dataset(test_path,vocab_path = vocab_dir,ues_word = False,pad_size = sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1639dc2b",
   "metadata": {},
   "source": [
    "## 4.2 Dataset Iterater构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1343ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetIterater(object):\n",
    "    def __init__(self, batches, batch_size, device):\n",
    "        self.batch_size = batch_size\n",
    "        self.batches = batches\n",
    "        self.n_batches = len(batches) // batch_size\n",
    "        self.residue = False  # 记录batch数量是否为整数\n",
    "        if len(batches) % self.n_batches != 0:\n",
    "            self.residue = True\n",
    "        self.index = 0\n",
    "        self.device = device\n",
    "\n",
    "    def _to_tensor(self, datas):\n",
    "        x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n",
    "        y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n",
    "\n",
    "        # pad前的长度(超过pad_size的设为pad_size)\n",
    "        # seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n",
    "        return x, y\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.residue and self.index == self.n_batches:\n",
    "            batches = self.batches[self.index * self.batch_size: len(self.batches)]\n",
    "            self.index += 1\n",
    "            batches = self._to_tensor(batches)\n",
    "            return batches\n",
    "\n",
    "        elif self.index >= self.n_batches:\n",
    "            self.index = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n",
    "            self.index += 1\n",
    "            batches = self._to_tensor(batches)\n",
    "            return batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.residue:\n",
    "            return self.n_batches + 1\n",
    "        else:\n",
    "            return self.n_batches\n",
    "\n",
    "train_data_iter = DatasetIterater(train_data, sequence_length, device)\n",
    "val_data_iter = DatasetIterater(val_data, sequence_length, device)\n",
    "test_data_iter = DatasetIterater(test_data, sequence_length, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c960b3a",
   "metadata": {},
   "source": [
    "# 5、模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb9ca46",
   "metadata": {},
   "source": [
    "## 5.1 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b98bfb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2           # number of classes\n",
    "filter_sizes = [3, 4, 5]  # n-gram windows\n",
    "num_filters = 100         # number of filters\n",
    "learning_rate = 0.008     # learning rate\n",
    "dropout = 0.3             # dropout rate\n",
    "# ### 模型定义\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.num_filters_total = num_filters * len(filter_sizes)\n",
    "#         self.W = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        self.W = nn.Embedding.from_pretrained(embedding_pretrained, freeze=True)\n",
    "        self.Weight = nn.Linear(self.num_filters_total, num_classes, bias=False)\n",
    "        self.Bias = nn.Parameter(torch.ones([num_classes]))\n",
    "        self.filter_list = nn.ModuleList([nn.Conv2d(1, num_filters, (size, embedding_size)) for size in filter_sizes])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        embedded_chars = self.W(X) # [batch_size, sequence_length, sequence_length]\n",
    "        embedded_chars = embedded_chars.unsqueeze(1) # add channel(=1) [batch, channel(=1), sequence_length, embedding_size]\n",
    "\n",
    "        pooled_outputs = []\n",
    "        for i, conv in enumerate(self.filter_list):\n",
    "            # conv : [input_channel(=1), output_channel(=3), (filter_height, filter_width), bias_option]\n",
    "            h = F.relu(conv(embedded_chars))\n",
    "            # mp : ((filter_height, filter_width))\n",
    "            mp = nn.MaxPool2d((sequence_length - filter_sizes[i] + 1, 1))\n",
    "            # pooled : [batch_size(=6), output_height(=1), output_width(=1), output_channel(=3)]\n",
    "            pooled = mp(h).permute(0, 3, 2, 1)  # 排列，将tensor的维度换位\n",
    "            pooled_outputs.append(pooled)\n",
    "\n",
    "        h_pool = torch.cat(pooled_outputs, len(filter_sizes)) # [batch_size(=6), output_height(=1), output_width(=1), output_channel(=3) * 3]\n",
    "        h_pool = self.dropout(h_pool)\n",
    "        h_pool_flat = torch.reshape(h_pool, [-1, self.num_filters_total]) # [batch_size(=6), output_height * output_width * (output_channel * 3)]\n",
    "        model = self.Weight(h_pool_flat) + self.Bias # [batch_size, num_classes]\n",
    "        return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4a799",
   "metadata": {},
   "source": [
    "## 5.2 模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bd947c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network(model, method='xavier', exclude='embedding', seed=123):\n",
    "    for name, w in model.named_parameters():\n",
    "        print(name)\n",
    "        if exclude not in name:\n",
    "            if 'weight' in name:\n",
    "                if method == 'xavier':\n",
    "                    nn.init.xavier_normal_(w)\n",
    "                elif method == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(w)\n",
    "                else:\n",
    "                    nn.init.normal_(w)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(w, 0)\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "667998ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias\n",
      "W.weight\n",
      "Weight.weight\n",
      "filter_list.0.weight\n",
      "filter_list.0.bias\n",
      "filter_list.1.weight\n",
      "filter_list.1.bias\n",
      "filter_list.2.weight\n",
      "filter_list.2.bias\n"
     ]
    }
   ],
   "source": [
    "model = TextCNN()\n",
    "init_network(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d360a68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextCNN(\n",
       "  (W): Embedding(2270, 300)\n",
       "  (Weight): Linear(in_features=300, out_features=2, bias=False)\n",
       "  (filter_list): ModuleList(\n",
       "    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1))\n",
       "    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1))\n",
       "    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda0531",
   "metadata": {},
   "source": [
    "## 5.3 模型训练、验证、测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7986f305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "\n",
    "def train(model, train_iter, dev_iter, test_iter):\n",
    "    start_time = time.time() \n",
    "    model.train()  #启用 Batch Normalization 和 Dropout\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    total_batch = 0  # 记录进行到多少batch\n",
    "    dev_best_loss = float('inf')\n",
    "    last_improve = 0  # 记录上次验证集loss下降的batch数\n",
    "    flag = False  # 记录是否很久没有效果提升\n",
    "    writer = SummaryWriter(log_dir=log_path + '/' + time.strftime('%m-%d_%H.%M', time.localtime()))\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, num_epochs))\n",
    "        # scheduler.step() # 学习率衰减\n",
    "        for i, (trains, labels) in enumerate(train_iter):\n",
    "            outputs = model(trains)\n",
    "            model.zero_grad()\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if total_batch % 100 == 0:\n",
    "                # 每多少轮输出在训练集和验证集上的效果\n",
    "                true = labels.data.cpu()\n",
    "                predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "                train_acc = metrics.accuracy_score(true, predic)\n",
    "                dev_acc, dev_loss = evaluate(model, dev_iter)\n",
    "                if dev_loss < dev_best_loss:\n",
    "                    dev_best_loss = dev_loss\n",
    "                    \n",
    "                    # 模型文件保存\n",
    "                    torch.save(model.state_dict(), save_path)\n",
    "                    improve = '*'\n",
    "                    last_improve = total_batch\n",
    "                    \n",
    "                    traced_model = torch.jit.trace(model, trains)\n",
    "                    # 保存模型 java调用\n",
    "                    current_time = datetime.datetime.now().strftime('%Y%m%d%s')\n",
    "                    torch.jit.save(traced_model, save_path[:-5] + \"_java_\"  + \".pt\")\n",
    "                else:\n",
    "                    improve = ''\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n",
    "                writer.add_scalar(\"loss/train\", loss.item(), total_batch)\n",
    "                writer.add_scalar(\"loss/dev\", dev_loss, total_batch)\n",
    "                writer.add_scalar(\"acc/train\", train_acc, total_batch)\n",
    "                writer.add_scalar(\"acc/dev\", dev_acc, total_batch)\n",
    "                model.train()\n",
    "            total_batch += 1\n",
    "            if total_batch - last_improve > require_improvement:\n",
    "                # 验证集loss超过指定batch没下降，结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            break\n",
    "    writer.close()\n",
    "    test(model, test_iter)\n",
    "\n",
    "\n",
    "def test(model, test_iter):\n",
    "    # test\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    test_acc, test_loss, test_report, test_confusion = evaluate(model, test_iter, test=True)\n",
    "    msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%}'\n",
    "    print(msg.format(test_loss, test_acc))\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(test_report)\n",
    "    print(\"Confusion Matrix...\")\n",
    "    print(test_confusion)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "\n",
    "def evaluate(model, data_iter, test=False):\n",
    "    model.eval()  #不启用 Batch Normalization 和 Dropout\n",
    "    loss_total = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_iter:\n",
    "            outputs = model(texts)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss_total += loss\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "    if test:\n",
    "        report = metrics.classification_report(labels_all, predict_all, target_names=class_list, digits=4)\n",
    "        confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "        return acc, loss_total / len(data_iter), report, confusion\n",
    "    return acc, loss_total / len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d07cdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Iter:      0,  Train Loss:  0.69,  Train Acc: 40.62%,  Val Loss:  0.76,  Val Acc: 65.98%,  Time: 0:00:02 *\n",
      "Epoch [2/10]\n",
      "Iter:    100,  Train Loss:  0.26,  Train Acc: 90.62%,  Val Loss:  0.29,  Val Acc: 87.70%,  Time: 0:00:20 *\n",
      "Epoch [3/10]\n",
      "Iter:    200,  Train Loss:  0.28,  Train Acc: 88.28%,  Val Loss:  0.31,  Val Acc: 87.58%,  Time: 0:00:41 \n",
      "No optimization for a long time, auto-stopping...\n",
      "Test Loss:   0.3,  Test Acc: 86.94%\n",
      "Precision, Recall and F1-Score...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad     0.8786    0.9310    0.9040      2115\n",
      "        good     0.8478    0.7493    0.7955      1085\n",
      "\n",
      "    accuracy                         0.8694      3200\n",
      "   macro avg     0.8632    0.8401    0.8498      3200\n",
      "weighted avg     0.8682    0.8694    0.8672      3200\n",
      "\n",
      "Confusion Matrix...\n",
      "[[1969  146]\n",
      " [ 272  813]]\n",
      "Time usage: 0:00:02\n"
     ]
    }
   ],
   "source": [
    "log_path = \"logs\"\n",
    "model_name = \"外卖评价\"\n",
    "\n",
    "save_path = 'saved_dict/'  + model_name + '.ckpt'\n",
    "require_improvement = 100 \n",
    "num_epochs = 10\n",
    "\n",
    "class_list = [\"bad\",\"good\"]\n",
    "train(model, train_data_iter, val_data_iter, test_data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec2e1f",
   "metadata": {},
   "source": [
    "# 6、模型加载及调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70805206",
   "metadata": {},
   "source": [
    "### 注：pt模型用于线上java调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8302f44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"saved_dict/外卖评价_java_.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c01579a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_jit = torch.jit.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75df3417",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.LongTensor([_[0] for _ in test_data]).to(device)\n",
    "\n",
    "y = np.array([_[1] for _ in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d36361e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model_jit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20a8eea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9936454 , 0.00635462],\n",
       "       [0.1552181 , 0.8447819 ],\n",
       "       [0.9862629 , 0.01373707],\n",
       "       ...,\n",
       "       [0.9759912 , 0.02400879],\n",
       "       [0.99881834, 0.0011816 ],\n",
       "       [0.18203264, 0.81796736]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = nn.Softmax(dim=1)(outputs).data.numpy()  #Softmax 变换\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3c849ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>预测为0概率</th>\n",
       "      <th>预测为1概率</th>\n",
       "      <th>真实类别</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.993645</td>\n",
       "      <td>0.006355</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.155218</td>\n",
       "      <td>0.844782</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.986263</td>\n",
       "      <td>0.013737</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.242531</td>\n",
       "      <td>0.757469</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.914644</td>\n",
       "      <td>0.085356</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     预测为0概率    预测为1概率  真实类别\n",
       "0  0.993645  0.006355     0\n",
       "1  0.155218  0.844782     1\n",
       "2  0.986263  0.013737     0\n",
       "3  0.242531  0.757469     1\n",
       "4  0.914644  0.085356     0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame({\"预测为0概率\":prob[:,0],\"预测为1概率\":prob[:,1],\"真实类别\":y})\n",
    "result.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd7e7352",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_excel(\"result/test_data_probability.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ad8ae2",
   "metadata": {},
   "source": [
    "# 7、模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020c40de",
   "metadata": {},
   "source": [
    "## 7.1 AUC评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3b9744f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___AUC___ : 0.9409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe113f537f0>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predit_prob_list = result[\"预测为1概率\"].values\n",
    "label_list = result[\"真实类别\"].values\n",
    "\n",
    "fpr, tpr, thersholds = roc_curve(label_list,predit_prob_list)\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"___AUC___ :\",round(roc_auc,4))\n",
    "\n",
    "plt.plot(fpr, tpr, 'k--', label='ROC (area = {})'.format(roc_auc), lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d388cc5e",
   "metadata": {},
   "source": [
    "## 7.2 accuracy、recall、precision、f1_score评估（含混淆矩阵图）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5631d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义切割阈值\n",
    "\n",
    "thresholds = np.arange(0, 1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d80df2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "912f2f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.0\n",
      "accuracy: 0.3390625\n",
      "Recall: 1.0\n",
      "Precision: 0.3390625\n",
      "f1_score: 0.5064177362893816\n",
      "\n",
      "\n",
      "Threshold: 0.1\n",
      "accuracy: 0.810625\n",
      "Recall: 0.9483870967741935\n",
      "Precision: 0.6516782773907537\n",
      "f1_score: 0.7725225225225225\n",
      "\n",
      "\n",
      "Threshold: 0.2\n",
      "accuracy: 0.8628125\n",
      "Recall: 0.9087557603686636\n",
      "Precision: 0.7435897435897436\n",
      "f1_score: 0.817917876399834\n",
      "\n",
      "\n",
      "Threshold: 0.30000000000000004\n",
      "accuracy: 0.87875\n",
      "Recall: 0.8599078341013825\n",
      "Precision: 0.7981180496150556\n",
      "f1_score: 0.8278615794143744\n",
      "\n",
      "\n",
      "Threshold: 0.4\n",
      "accuracy: 0.880625\n",
      "Recall: 0.8147465437788018\n",
      "Precision: 0.8300469483568075\n",
      "f1_score: 0.8223255813953487\n",
      "\n",
      "\n",
      "Threshold: 0.5\n",
      "accuracy: 0.869375\n",
      "Recall: 0.7493087557603687\n",
      "Precision: 0.8477580813347236\n",
      "f1_score: 0.7954990215264188\n",
      "\n",
      "\n",
      "Threshold: 0.6000000000000001\n",
      "accuracy: 0.8590625\n",
      "Recall: 0.6857142857142857\n",
      "Precision: 0.8711943793911007\n",
      "f1_score: 0.7674058793192368\n",
      "\n",
      "\n",
      "Threshold: 0.7000000000000001\n",
      "accuracy: 0.840625\n",
      "Recall: 0.6064516129032258\n",
      "Precision: 0.8879892037786775\n",
      "f1_score: 0.7207009857612267\n",
      "\n",
      "\n",
      "Threshold: 0.8\n",
      "accuracy: 0.8096875\n",
      "Recall: 0.4783410138248848\n",
      "Precision: 0.9234875444839857\n",
      "f1_score: 0.6302367941712204\n",
      "\n",
      "\n",
      "Threshold: 0.9\n",
      "accuracy: 0.7546875\n",
      "Recall: 0.2921658986175115\n",
      "Precision: 0.9491017964071856\n",
      "f1_score: 0.44679351656095845\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "j = 1\n",
    "accuracyList,recallList,precisionList,f1_scoreList = [],[],[],[]\n",
    "for i in thresholds:\n",
    "    y_pred = [predit_prob >i for predit_prob in predit_prob_list]\n",
    "#     plt.subplot(3,3,j)\n",
    "    j += 1\n",
    "    cnf_matrix = confusion_matrix(label_list, y_pred) #计算混淆矩阵\n",
    "\n",
    "    np.set_printoptions(precision = 2)\n",
    "    accuracy = (cnf_matrix[1,1]+cnf_matrix[0,0])/(cnf_matrix[1,1]+cnf_matrix[0,1]+cnf_matrix[0,0]+cnf_matrix[1,0])\n",
    "    recall = cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[1,0])\n",
    "    precision = cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[0,1])\n",
    "    f1_score = 2 * precision * recall / (recall + precision)\n",
    "    accuracyList.append(accuracy)\n",
    "    recallList.append(recall)\n",
    "    precisionList.append(precision)\n",
    "    f1_scoreList.append(f1_score)\n",
    "    print('Threshold:',i)\n",
    "    print('accuracy:', accuracy)\n",
    "    print('Recall:', recall)     \n",
    "    print('Precision:', precision)\n",
    "    print('f1_score:',f1_score)\n",
    "    class_names = [0, 1]\n",
    "#     plot_confusion_matrix(cnf_matrix, classes = class_names, title = 'Threshold>=%s' %i)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f2d2768",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalResult = pd.DataFrame({'threshold':thresholds,\n",
    "                   'accuracy':accuracyList,\n",
    "                   'recall':recallList,\n",
    "                   'precision':precisionList,\n",
    "                  'f1_score':f1_scoreList})\n",
    "evalResult.to_excel(\"result/test_data_evaluation.xlsx\",index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
